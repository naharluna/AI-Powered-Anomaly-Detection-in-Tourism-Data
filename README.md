# ğŸ§  AI-Powered Anomaly Detection in Tourism Data  
## Early Identification of Destination Overcrowding

This repository contains an end-to-end machine learning workflow for **detecting anomalies in tourism datasets** to identify early signs of **destination overcrowding**. The project explores both baseline ML models and an advanced hybrid model combining traditional and deep learning techniques to improve detection accuracy.

---

## ğŸš€ Features
- âœ”ï¸ Data preprocessing and feature engineering  
- âœ”ï¸ Baseline model using **CatBoost**  
- âœ”ï¸ Proposed hybrid model **TH-StackNet**  
- âœ”ï¸ Model comparison & evaluation  
- âœ”ï¸ Explainability with **SHAP analysis**  
- âœ”ï¸ Complete Jupyter Notebook included  

---

## ğŸ“ Project Structure

â”œâ”€â”€ AI_Powered_Anomaly_Detection_in_Tourism_Data.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ data/ (optional â€“ add your dataset here)


---

## ğŸ—ï¸ Model Architecture

This project uses two main modeling approaches:

---

# **1ï¸âƒ£ Baseline Model: CatBoost Classifier**

A powerful gradient-boosting algorithm designed for:
- Handling categorical + numerical data automatically  
- Fast training  
- High accuracy on tabular datasets  
- Built-in regularization to reduce overfitting  

**CatBoost Workflow:**
Input Data â†’ Encoding/Preprocessing â†’ CatBoost Training â†’ Prediction â†’ Anomaly Score


---

# **2ï¸âƒ£ Proposed Hybrid Model: TH-StackNet**

The proposed model **TH-StackNet** (Tourism Hybrid Stacked Network) combines  
traditional ML algorithms with a neural network to maximize anomaly detection accuracy.

The architecture follows a **stacked ensemble** design:

             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚     Input Feature Set      â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                 â”‚                  â”‚
          â–¼                 â–¼                  â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ML Model 1   â”‚   â”‚ ML Model 2   â”‚    â”‚ Neural Network â”‚
  â”‚ (e.g., RF)   â”‚   â”‚ (e.g., XGB)  â”‚    â”‚   (Dense NN)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
          â”‚                 â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼             â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   Stacking Layer        â”‚
               â”‚  (Meta-Learner Model)   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  Final Output  â”‚
                     â”‚ (Anomaly Flag) â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     

---

## ğŸ” TH-StackNet Components

### **ğŸ”¹ Level-1 Models (Base Learners)**
- Random Forest  
- XGBoost  
- Neural Network (Fully connected layers)

These models learn independently and extract different feature relationships.

### **ğŸ”¹ Level-2 Model (Meta-Learner)**
A lightweight ML model (e.g., Logistic Regression or LightGBM) that:
- Takes predictions from Level-1 models  
- Learns optimal combination weights  
- Produces the final anomaly classification  

---

## âš™ï¸ Neural Network Sub-Architecture
Input Layer
â†“
Dense (64 units, ReLU)
â†“
Dropout (0.3)
â†“
Dense (32 units, ReLU)
â†“
Dropout (0.2)
â†“
Output Layer (Sigmoid)



---

## ğŸ¯ Why TH-StackNet Works Better
- Ensemble reduces variance and bias  
- Neural network captures non-linear patterns  
- ML models capture tree-based interactions  
- Meta-learner blends strengths of all models  
- More stable and robust for anomaly detection

---

## ğŸ“Œ Summary Table

| Component | Type | Purpose |
|----------|------|---------|
| CatBoost | Baseline | Benchmark model |
| RF + XGB | Base Learners | Tree-based feature interactions |
| Neural Network | Base Learner | Non-linear pattern extraction |
| Meta-Learner | Final Layer | Combines all model outputs |

---


---

## ğŸ“Š Explainability (SHAP)
The notebook includes **SHAP value analysis** to understand:
- Key feature contributions  
- How model decisions vary  
- Global vs. local interpretability  

---

